{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6516ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import mock_d2l_jax as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfa9f9",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc04260",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:43.476989Z",
     "iopub.status.busy": "2022-07-13T08:07:43.476371Z",
     "iopub.status.idle": "2022-07-13T08:07:45.469658Z",
     "shell.execute_reply": "2022-07-13T08:07:45.468685Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import jax\n",
    "from jax import numpy as jnp, random, grad, vmap, jit\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "# from d2l import jax as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be14c68",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "When we implemented linear regression from scratch\n",
    "in :numref:`sec_linear_scratch`,\n",
    "we defined our model parameters explicitly\n",
    "and coded up the calculations to produce output\n",
    "using basic linear algebra operations.\n",
    "You *should* know how to do this.\n",
    "But once your models get more complex,\n",
    "and once you have to do this nearly every day,\n",
    "you will be glad for the assistance.\n",
    "The situation is similar to coding up your own blog from scratch.\n",
    "Doing it once or twice is rewarding and instructive,\n",
    "but you would be a lousy web developer\n",
    "if you spent a month reinventing the wheel.\n",
    "\n",
    "For standard operations,\n",
    "we can [**use a framework's predefined layers,**]\n",
    "which allow us to focus\n",
    "on the layers used to construct the model\n",
    "rather than worrying about their implementation.\n",
    "Recall the architecture of a single-layer network\n",
    "as described in :numref:`fig_single_neuron`.\n",
    "The layer is called *fully connected*,\n",
    "since each of its inputs is connected\n",
    "to each of its outputs\n",
    "by means of a matrix-vector multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34912ce8",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In PyTorch, the fully connected layer is defined in `Linear` and `LazyLinear` (available since version 1.8.0) classes. \n",
    "The later\n",
    "allows users to *only* specify\n",
    "the output dimension,\n",
    "while the former\n",
    "additionally asks for\n",
    "how many inputs go into this layer.\n",
    "Specifying input shapes is inconvenient,\n",
    "which may require nontrivial calculations\n",
    "(such as in convolutional layers).\n",
    "Thus, for simplicity we will use such \"lazy\" layers\n",
    "whenever we can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da158dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:45.474567Z",
     "iopub.status.busy": "2022-07-13T08:07:45.473814Z",
     "iopub.status.idle": "2022-07-13T08:07:45.479048Z",
     "shell.execute_reply": "2022-07-13T08:07:45.478240Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):  # @save\n",
    "    lr: float\n",
    "\n",
    "    def setup(self):\n",
    "        # self.save_hyperparameters()\n",
    "        self.net = nn.Dense(1, kernel_init=nn.initializers.normal(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dfbc1",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "In the `forward` method, we just invoke the built-in `__call__` function of the predefined layers to compute the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd5f102",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:45.482607Z",
     "iopub.status.busy": "2022-07-13T08:07:45.481926Z",
     "iopub.status.idle": "2022-07-13T08:07:45.486322Z",
     "shell.execute_reply": "2022-07-13T08:07:45.485512Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def __call__(self, X):\n",
    "    return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768e04f",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Defining the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331c340",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**The `MSELoss` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).**]\n",
    "By default, `MSELoss` returns the average loss over examples.\n",
    "It is faster (and easier to use) than implementing our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f6d7f6",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:45.490101Z",
     "iopub.status.busy": "2022-07-13T08:07:45.489440Z",
     "iopub.status.idle": "2022-07-13T08:07:45.493690Z",
     "shell.execute_reply": "2022-07-13T08:07:45.492880Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def loss(self, params, x, y):\n",
    "    # output needs to be be a scalar (todo: maybe there's a better way to do this)\n",
    "    return jnp.mean(vmap(optax.l2_loss)(self.apply(params, x), y), axis=0).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b9af9",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f2213",
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Minibatch SGD is a standard tool\n",
    "for optimizing neural networks\n",
    "and thus PyTorch supports it alongside a number of\n",
    "variations on this algorithm in the `optim` module.\n",
    "When we (**instantiate an `SGD` instance,**)\n",
    "we specify the parameters to optimize over,\n",
    "obtainable from our model via `self.parameters()`,\n",
    "and the learning rate (`self.lr`)\n",
    "required by our optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dfed7af",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:45.497036Z",
     "iopub.status.busy": "2022-07-13T08:07:45.496660Z",
     "iopub.status.idle": "2022-07-13T08:07:45.500921Z",
     "shell.execute_reply": "2022-07-13T08:07:45.500107Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return optax.sgd(self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b993efa",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "## Training\n",
    "\n",
    "You might have noticed that expressing our model through\n",
    "high-level APIs of a deep learning framework\n",
    "requires fewer lines of code.\n",
    "We did not have to allocate parameters individually,\n",
    "define our loss function, or implement minibatch SGD.\n",
    "Once we start working with much more complex models,\n",
    "the advantages of the high-level API will grow considerably.\n",
    "Now that we have all the basic pieces in place,\n",
    "[**the training loop itself is the same\n",
    "as the one we implemented from scratch.**]\n",
    "So we just call the `fit` method (introduced in :numref:`oo-design-training`),\n",
    "which relies on the implementation of the `fit_epoch` method\n",
    "in :numref:`sec_linear_scratch`,\n",
    "to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e797cdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:45.504035Z",
     "iopub.status.busy": "2022-07-13T08:07:45.503763Z",
     "iopub.status.idle": "2022-07-13T08:07:46.857889Z",
     "shell.execute_reply": "2022-07-13T08:07:46.856955Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flax.training.train_state import TrainState\n",
    "\n",
    "model = LinearRegression(0.03)\n",
    "data = d2l.SyntheticRegressionData(random.PRNGKey(0), w=jnp.array([2, -3.4]), b=4.2)\n",
    "params = model.init(random.PRNGKey(1), (2, 1))\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=model.configure_optimizers())\n",
    "\n",
    "grad_fn = grad(model.loss)\n",
    "\n",
    "for _ in range(3):    \n",
    "    for batch in data.train_dataloader():\n",
    "        # print(len(batch), batch[0].shape, batch[1].shape) \n",
    "        grads = grad_fn(state.params, batch[0], batch[1])\n",
    "        state = state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef901e",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "Below, we\n",
    "[**compare the model parameters learned\n",
    "by training on finite data\n",
    "and the actual parameters**]\n",
    "that generated our dataset.\n",
    "To access parameters,\n",
    "we access the weights and bias\n",
    "of the layer that we need.\n",
    "As in our implementation from scratch,\n",
    "note that our estimated parameters\n",
    "are close to their true counterparts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d30c136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T08:07:46.862076Z",
     "iopub.status.busy": "2022-07-13T08:07:46.861386Z",
     "iopub.status.idle": "2022-07-13T08:07:46.869688Z",
     "shell.execute_reply": "2022-07-13T08:07:46.868712Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: [ 0.0736717  -0.17495131]\n",
      "error in estimating b: [0.23346329]\n"
     ]
    }
   ],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def get_w_b(self, state):\n",
    "    net = state.params['params']['net']\n",
    "    return net['kernel'], net['bias']\n",
    "\n",
    "w, b = model.get_w_b(state)\n",
    "print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e1ce3",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "## Summary\n",
    "\n",
    "This section contains the first\n",
    "implementation of a deep network (in this book)\n",
    "to tap into the conveniences afforded\n",
    "by modern deep learning frameworks,\n",
    "such as Gluon `Chen.Li.Li.ea.2015`, \n",
    "JAX :cite:`Frostig.Johnson.Leary.2018`, \n",
    "PyTorch :cite:`Paszke.Gross.Massa.ea.2019`, \n",
    "and Tensorflow :cite:`Abadi.Barham.Chen.ea.2016`.\n",
    "We used framework defaults for loading data, defining a layer,\n",
    "a loss function, an optimizer and a training loop.\n",
    "Whenever the framework provides all necessary features,\n",
    "it's generally a good idea to use them,\n",
    "since the library implementations of these components\n",
    "tend to be heavily optimized for performance\n",
    "and properly tested for reliability.\n",
    "At the same time, try not to forget\n",
    "that these modules *can* be implemented directly.\n",
    "This is especially important for aspiring researchers\n",
    "who wish to live on the bleeding edge of model development,\n",
    "where you will be inventing new components\n",
    "that cannot possibly exist in any current library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960c87e",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In PyTorch, the `data` module provides tools for data processing,\n",
    "the `nn` module defines a large number of neural network layers and common loss functions.\n",
    "We can initialize the parameters by replacing their values\n",
    "with methods ending with `_`.\n",
    "Note that we need to specify the input dimensions of the network.\n",
    "While this is trivial for now, it can have significant knock-on effects\n",
    "when we want to design complex networks with many layers.\n",
    "Careful considerations of how to parametrize these networks\n",
    "is needed to allow portability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870626f5",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. How would you need to change the learning rate if you replace the aggregate loss over the minibatch\n",
    "   with an average over the loss on the minibatch?\n",
    "1. Review the framework documentation to see which loss functions are provided. In particular,\n",
    "   replace the squared loss with Huber's robust loss function. That is, use the loss function\n",
    "   $$l(y,y') = \\begin{cases}|y-y'| -\\frac{\\sigma}{2} & \\text{ if } |y-y'| > \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\text{ otherwise}\\end{cases}$$\n",
    "1. How do you access the gradient of the weights of the model?\n",
    "1. How does the solution change if you change the learning rate and the number of epochs? Does it keep on improving?\n",
    "1. How does the solution change as you change the amount of data generated?\n",
    "    1. Plot the estimation error for $\\hat{\\mathbf{w}} - \\mathbf{w}$ and $\\hat{b} - b$ as a function of the amount of data. Hint: increase the amount of data logarithmically rather than linearly, i.e., 5, 10, 20, 50, ..., 10,000 rather than 1,000, 2,000, ..., 10,000.\n",
    "    2. Why is the suggestion in the hint appropriate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11264af",
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/45)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "626d743d6476408aa1b36c3ff0d1f9d9d03e37c6879626ddfcdd13d658004bbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
