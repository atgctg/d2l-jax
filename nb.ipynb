{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import inspect\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, random, grad, vmap, jit\n",
    "from flax import linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\n",
    "\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    backend_inline.set_matplotlib_formats(\"svg\")\n",
    "\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\n",
    "\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams[\"figure.figsize\"] = figsize\n",
    "\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\n",
    "\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "\n",
    "def plot(\n",
    "    X,\n",
    "    Y=None,\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    legend=None,\n",
    "    xlim=None,\n",
    "    ylim=None,\n",
    "    xscale=\"linear\",\n",
    "    yscale=\"linear\",\n",
    "    fmts=(\"-\", \"m--\", \"g-.\", \"r:\"),\n",
    "    figsize=(3.5, 2.5),\n",
    "    axes=None,\n",
    "):\n",
    "    \"\"\"Plot data points.\n",
    "\n",
    "    Defined in :numref:`sec_calculus`\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else plt.gca()\n",
    "\n",
    "    # Return True if `X` (tensor or list) has 1 axis\n",
    "    def has_one_axis(X):\n",
    "        return (\n",
    "            hasattr(X, \"ndim\")\n",
    "            and X.ndim == 1\n",
    "            or isinstance(X, list)\n",
    "            and not hasattr(X[0], \"__len__\")\n",
    "        )\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "\n",
    "# 3.2\n",
    "\n",
    "\n",
    "def add_to_class(Class):  # @save\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class HyperParameters:  # @save\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        \"\"\"Save function arguments into class attributes.\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {\n",
    "            k: v\n",
    "            for k, v in local_vars.items()\n",
    "            if k not in set(ignore + [\"self\"]) and not k.startswith(\"_\")\n",
    "        }\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "class ProgressBoard(HyperParameters):\n",
    "    \"\"\"Plot data points in animation.\n",
    "\n",
    "    Defined in :numref:`sec_oo-design`\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xlabel=None,\n",
    "        ylabel=None,\n",
    "        xlim=None,\n",
    "        ylim=None,\n",
    "        xscale=\"linear\",\n",
    "        yscale=\"linear\",\n",
    "        ls=[\"-\", \"--\", \"-.\", \":\"],\n",
    "        colors=[\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        fig=None,\n",
    "        axes=None,\n",
    "        figsize=(3.5, 2.5),\n",
    "        display=True,\n",
    "    ):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def draw(self, x, y, label, every_n=1):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        Point = collections.namedtuple(\"Point\", [\"x\", \"y\"])\n",
    "        if not hasattr(self, \"raw_points\"):\n",
    "            self.raw_points = collections.OrderedDict()\n",
    "            self.data = collections.OrderedDict()\n",
    "        if label not in self.raw_points:\n",
    "            self.raw_points[label] = []\n",
    "            self.data[label] = []\n",
    "        points = self.raw_points[label]\n",
    "        line = self.data[label]\n",
    "        points.append(Point(x, y))\n",
    "        if len(points) != every_n:\n",
    "            return\n",
    "        mean = lambda x: sum(x) / len(x)\n",
    "        line.append(Point(mean([p.x for p in points]), mean([p.y for p in points])))\n",
    "        points.clear()\n",
    "        if not self.display:\n",
    "            return\n",
    "        use_svg_display()\n",
    "        if self.fig is None:\n",
    "            self.fig = plt.figure(figsize=self.figsize)\n",
    "        plt_lines, labels = [], []\n",
    "        for (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):\n",
    "            plt_lines.append(\n",
    "                plt.plot([p.x for p in v], [p.y for p in v], linestyle=ls, color=color)[\n",
    "                    0\n",
    "                ]\n",
    "            )\n",
    "            labels.append(k)\n",
    "        axes = self.axes if self.axes else plt.gca()\n",
    "        if self.xlim:\n",
    "            axes.set_xlim(self.xlim)\n",
    "        if self.ylim:\n",
    "            axes.set_ylim(self.ylim)\n",
    "        if not self.xlabel:\n",
    "            self.xlabel = self.x\n",
    "        axes.set_xlabel(self.xlabel)\n",
    "        axes.set_ylabel(self.ylabel)\n",
    "        axes.set_xscale(self.xscale)\n",
    "        axes.set_yscale(self.yscale)\n",
    "        axes.legend(plt_lines, labels)\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"Defined in :numref:`sec_oo-design`\"\"\"\n",
    "\n",
    "    plot_train_per_epoch: int = 2\n",
    "    plot_valid_per_epoch: int = 1\n",
    "    board: ProgressBoard = ProgressBoard()\n",
    "    training: bool = False\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X):\n",
    "        assert hasattr(self, \"net\"), \"Neural network is defined\"\n",
    "        return self.net(X)\n",
    "\n",
    "    def __call__(self, X, *args, **kwargs):\n",
    "        if kwargs and \"training\" in kwargs:\n",
    "            self.training = kwargs[\"training\"]\n",
    "        return self.forward(X, *args)\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, \"trainer\"), \"Trainer is not inited\"\n",
    "        self.board.xlabel = \"epoch\"\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value, (\"train_\" if train else \"val_\") + key, every_n=int(n))\n",
    "\n",
    "    def training_step(self, params, batch):\n",
    "        loss, grads = jax.value_and_grad(self.loss)(\n",
    "            params, *batch[:-1], batch[-1]\n",
    "        )\n",
    "        self.plot(\"loss\", loss, train=True)\n",
    "        return loss, grads\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot(\"loss\", l, train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Defined in :numref:`sec_classification`\"\"\"\n",
    "        return optax.sgd(learning_rate=self.lr)\n",
    "\n",
    "\n",
    "# 3.3\n",
    "\n",
    "\n",
    "class DataModule(HyperParameters):\n",
    "    \"\"\"Defined in :numref:`sec_oo-design`\"\"\"\n",
    "\n",
    "    def __init__(self, root=\"../data\"):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        shuffle_buffer = tensors[0].shape[0] if train else 1\n",
    "        return tfds.as_numpy(\n",
    "            tf.data.Dataset.from_tensor_slices(tensors)\n",
    "            .shuffle(buffer_size=shuffle_buffer)\n",
    "            .batch(self.batch_size)\n",
    "        )\n",
    "\n",
    "\n",
    "class Trainer(HyperParameters):\n",
    "    \"\"\"Defined in :numref:`sec_oo-design`\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, params, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.state = TrainState.create(apply_fn=model.apply, params=params, tx=model.configure_optimizers())\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def prepare_batch(self, batch):\n",
    "        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n",
    "        return batch\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n",
    "        self.model.training = True\n",
    "\n",
    "\n",
    "        for batch in self.train_dataloader:\n",
    "            # with tf.GradientTape() as tape:\n",
    "            #     loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            # grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            loss, grads = self.model.training_step(self.state.params, self.prepare_batch(batch))\n",
    "\n",
    "            # todo: clip\n",
    "            # if self.gradient_clip_val > 0:\n",
    "            #     grads = self.clip_gradients(self.gradient_clip_val, grads)\n",
    "\n",
    "            # self.optim.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "            self.state = self.state.apply_gradients(grads=grads)\n",
    "\n",
    "            self.train_batch_idx += 1\n",
    "\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "\n",
    "        self.model.training = False\n",
    "        # for batch in self.val_dataloader:\n",
    "        #     self.model.validation_step(self.prepare_batch(batch))\n",
    "        #     self.val_batch_idx += 1\n",
    "\n",
    "    # todo: clip\n",
    "    # def clip_gradients(self, grad_clip_val, grads):\n",
    "    #     \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "    #     grad_clip_val = tf.constant(grad_clip_val, dtype=tf.float32)\n",
    "    #     new_grads = [tf.convert_to_tensor(grad) if isinstance(\n",
    "    #         grad, tf.IndexedSlices) else grad for grad in grads]\n",
    "    #     norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)) for grad in new_grads))\n",
    "    #     if tf.greater(norm, grad_clip_val):\n",
    "    #         for i, grad in enumerate(new_grads):\n",
    "    #             new_grads[i] = grad * grad_clip_val / norm\n",
    "    #         return new_grads\n",
    "    #     return grads\n",
    "\n",
    "\n",
    "# Use TensorFlow's dataloader, since JAX doesn't have one\n",
    "class SyntheticRegressionData(DataModule, HyperParameters):  # @save\n",
    "    def __init__(\n",
    "        self, key, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.X = jax.random.normal(key1, (n, len(w)))\n",
    "        noise = jax.random.normal(key2, (n, 1)) * noise\n",
    "        self.y = jnp.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "\n",
    "# Since JAX doesn't have a dataloader, we'll use PyTorch's\n",
    "# def load_array(data_arrays, batch_size, is_train=True):  # @save\n",
    "#     \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "#     # convert JAX arrays to PyTorch tensors\n",
    "#     data_arrays = (torch.from_numpy(np.array(x)) for x in data_arrays)\n",
    "\n",
    "#     dataset = data.TensorDataset(*data_arrays)\n",
    "#     return data.DataLoader(dataset, batch_size, shuffle=is_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(Module):  # @save\n",
    "    lr: float = 0.01\n",
    "\n",
    "    def setup(self):\n",
    "        # self.save_hyperparameters()\n",
    "        self.net = nn.Dense(1, kernel_init=nn.initializers.normal(0.01))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.net(X)\n",
    "        \n",
    "    def loss(self, params, x, y):\n",
    "        # output needs to be be a scalar (todo: maybe there's a better way to do this)\n",
    "        return jnp.mean(vmap(optax.l2_loss)(self.apply(params, x), y), axis=0).squeeze()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optax.sgd(self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model = LinearRegression(lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=tf.constant([2, -3.4]), b=4.2)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(0.03)\n",
    "data = SyntheticRegressionData(random.PRNGKey(0), w=jnp.array([2, -3.4]), b=4.2)\n",
    "params = model.init(random.PRNGKey(1), (2, 1))\n",
    "trainer = Trainer(max_epochs=3)\n",
    "trainer.fit(params, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: [ 0.6571226 -1.3016284]\n",
      "error in estimating b: [1.6295347]\n"
     ]
    }
   ],
   "source": [
    "# @add_to_class(LinearRegression)  #@save\n",
    "# def get_w_b(self):\n",
    "#     return (self.state()[0], self.get_weights()[1])\n",
    "\n",
    "# w, b = model.get_w_b()\n",
    "w, b = trainer.state.params['params']['net']['kernel'], trainer.state.params['params']['net']['bias']\n",
    "print(f'error in estimating w: {data.w - jnp.reshape(w, data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "626d743d6476408aa1b36c3ff0d1f9d9d03e37c6879626ddfcdd13d658004bbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
