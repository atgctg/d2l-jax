{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "793e4df5",
      "metadata": {},
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1cba53b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import mock_d2l_jax as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "092f0fcb",
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "The function below takes three arguments corresponding to the number\n",
        "of convolutional layers `num_convs`, the number of input channels `in_channels`\n",
        "and the number of output channels `out_channels`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab330311",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:47:32.657514Z",
          "iopub.status.busy": "2022-07-13T08:47:32.656511Z",
          "iopub.status.idle": "2022-07-13T08:47:34.742455Z",
          "shell.execute_reply": "2022-07-13T08:47:34.741412Z"
        },
        "origin_pos": 5,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax import numpy as jnp, random, grad, vmap, jit\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "# from d2l import jax as w\n",
        "\n",
        "\n",
        "def vgg_block(num_convs, out_channels):\n",
        "    layers = []\n",
        "    for _ in range(num_convs):\n",
        "        layers.append(nn.Conv(out_channels, kernel_size=(3, 3), padding=(1, 1)))\n",
        "        layers.append(nn.relu)\n",
        "    layers.append(lambda x: nn.max_pool(x, window_shape=(2, 2), strides=(2, 2)))\n",
        "    return nn.Sequential(layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "100eed9f",
      "metadata": {
        "origin_pos": 7
      },
      "source": [
        "## [**VGG Network**]\n",
        ":label:`subsec_vgg-network`\n",
        "\n",
        "Like AlexNet and LeNet, \n",
        "the VGG Network can be partitioned into two parts:\n",
        "the first consisting mostly of convolutional and pooling layers\n",
        "and the second consisting of fully connected layers that are identical to those in AlexNet. \n",
        "The key difference is \n",
        "that the convolutional layers are grouped in nonlinear transformations that \n",
        "leave the dimensonality unchanged, followed by a resolution-reduction step, as \n",
        "depicted in :numref:`fig_vgg`. \n",
        "\n",
        "![From AlexNet to VGG that is designed from building blocks.](http://d2l.ai/_images/vgg.svg)\n",
        ":width:`400px`\n",
        ":label:`fig_vgg`\n",
        "\n",
        "The convolutional part of the network connects several VGG blocks from :numref:`fig_vgg` (also defined in the `vgg_block` function)\n",
        "in succession. This grouping of convolutions is a pattern that has \n",
        "remained almost unchanged over the past decade, although the specific choice of \n",
        "operations has undergone considerable modifications. \n",
        "The variable `conv_arch` consists of a list of tuples (one per block),\n",
        "where each contains two values: the number of convolutional layers\n",
        "and the number of output channels,\n",
        "which are precisely the arguments required to call\n",
        "the `vgg_block` function. As such, VGG defines a *family* of networks rather than just \n",
        "a specific manifestation. To build a specific network we simply iterate over `arch` to compose the blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32fd82d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:47:34.748204Z",
          "iopub.status.busy": "2022-07-13T08:47:34.747817Z",
          "iopub.status.idle": "2022-07-13T08:47:34.754332Z",
          "shell.execute_reply": "2022-07-13T08:47:34.753376Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "class VGG(d2l.Classifier):\n",
        "    arch: list\n",
        "    lr: float = 0.1\n",
        "    num_classes: int = 10\n",
        "    \n",
        "    def setup(self):\n",
        "        conv_blks = []\n",
        "        for (num_convs, out_channels) in self.arch:\n",
        "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
        "        self.net = nn.Sequential([\n",
        "            *conv_blks, d2l.flatten,\n",
        "            nn.Dense(4096), nn.relu, nn.Dropout(0.5, deterministic=False),\n",
        "            nn.Dense(4096), nn.relu, nn.Dropout(0.5, deterministic=False),\n",
        "            nn.Dense(self.num_classes)])\n",
        "        # self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbeed4f8",
      "metadata": {
        "origin_pos": 9
      },
      "source": [
        "The original VGG network had 5 convolutional blocks,\n",
        "among which the first two have one convolutional layer each\n",
        "and the latter three contain two convolutional layers each.\n",
        "The first block has 64 output channels\n",
        "and each subsequent block doubles the number of output channels,\n",
        "until that number reaches 512.\n",
        "Since this network uses 8 convolutional layers\n",
        "and 3 fully connected layers, it is often called VGG-11.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d6b67f8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                   VGG Summary                                   </span>\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> path                  </span>┃<span style=\"font-weight: bold\"> outputs                </span>┃<span style=\"font-weight: bold\"> params                       </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Inputs                │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,224,224,1]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_0/layers_0 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,224,224,64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,1,64]    │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">640 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.6 KB)</span>                 │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_0          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,112,112,64]  │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_1/layers_0 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,112,112,128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,64,128]  │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">73,856 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(295.4 KB)</span>            │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_1          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,56,56,128]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,4096]        │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,10]          │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[10]            │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4096,10]     │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">40,970 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(163.9 KB)</span>            │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_2/layers_0 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,56,56,256]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,128,256] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">295,168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.2 MB)</span>             │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_2/layers_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,56,56,256]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,256,256] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">590,080 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.4 MB)</span>             │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_2          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,28,28,256]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3/layers_0 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,28,28,512]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,256,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">1,180,160 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(4.7 MB)</span>           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3/layers_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,28,28,512]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,512,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">2,359,808 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(9.4 MB)</span>           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,14,14,512]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_4/layers_0 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,14,14,512]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,512,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">2,359,808 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(9.4 MB)</span>           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_4/layers_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,14,14,512]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]           │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,512,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">2,359,808 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(9.4 MB)</span>           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_4          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,7,7,512]     │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_6          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,4096]        │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4096]          │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[25088,4096]  │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">102,764,544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(411.1 MB)</span>       │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,4096]        │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,4096]        │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4096]          │\n",
              "│                       │                        │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4096,4096]   │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ <span style=\"font-weight: bold\">16,781,312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(67.1 MB)</span>         │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net                   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,10]          │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ VGG                   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,10]          │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│<span style=\"font-weight: bold\">                       </span>│<span style=\"font-weight: bold\">                  Total </span>│<span style=\"font-weight: bold\"> 128,806,154 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(515.2 MB)</span><span style=\"font-weight: bold\">       </span>│\n",
              "└───────────────────────┴────────────────────────┴──────────────────────────────┘\n",
              "<span style=\"font-weight: bold\">                                                                                 </span>\n",
              "<span style=\"font-weight: bold\">                    Total Parameters: 128,806,154 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(515.2 MB)</span><span style=\"font-weight: bold\">                     </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                                   VGG Summary                                   \u001b[0m\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mpath                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                      \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Inputs                │ \u001b[2mfloat32\u001b[0m[1,224,224,1]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_0/layers_0 │ \u001b[2mfloat32\u001b[0m[1,224,224,64]  │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,1,64]    │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m640 \u001b[0m\u001b[1;2m(2.6 KB)\u001b[0m                 │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_0          │ \u001b[2mfloat32\u001b[0m[1,112,112,64]  │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_1/layers_0 │ \u001b[2mfloat32\u001b[0m[1,112,112,128] │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,64,128]  │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m73,856 \u001b[0m\u001b[1;2m(295.4 KB)\u001b[0m            │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_1          │ \u001b[2mfloat32\u001b[0m[1,56,56,128]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11         │ \u001b[2mfloat32\u001b[0m[1,4096]        │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12         │ \u001b[2mfloat32\u001b[0m[1,10]          │ bias: \u001b[2mfloat32\u001b[0m[10]            │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[4096,10]     │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m40,970 \u001b[0m\u001b[1;2m(163.9 KB)\u001b[0m            │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_2/layers_0 │ \u001b[2mfloat32\u001b[0m[1,56,56,256]   │ bias: \u001b[2mfloat32\u001b[0m[256]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,128,256] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m295,168 \u001b[0m\u001b[1;2m(1.2 MB)\u001b[0m             │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_2/layers_2 │ \u001b[2mfloat32\u001b[0m[1,56,56,256]   │ bias: \u001b[2mfloat32\u001b[0m[256]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,256,256] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m590,080 \u001b[0m\u001b[1;2m(2.4 MB)\u001b[0m             │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_2          │ \u001b[2mfloat32\u001b[0m[1,28,28,256]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3/layers_0 │ \u001b[2mfloat32\u001b[0m[1,28,28,512]   │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,256,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m1,180,160 \u001b[0m\u001b[1;2m(4.7 MB)\u001b[0m           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3/layers_2 │ \u001b[2mfloat32\u001b[0m[1,28,28,512]   │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,512,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m2,359,808 \u001b[0m\u001b[1;2m(9.4 MB)\u001b[0m           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3          │ \u001b[2mfloat32\u001b[0m[1,14,14,512]   │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_4/layers_0 │ \u001b[2mfloat32\u001b[0m[1,14,14,512]   │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,512,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m2,359,808 \u001b[0m\u001b[1;2m(9.4 MB)\u001b[0m           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_4/layers_2 │ \u001b[2mfloat32\u001b[0m[1,14,14,512]   │ bias: \u001b[2mfloat32\u001b[0m[512]           │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[3,3,512,512] │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m2,359,808 \u001b[0m\u001b[1;2m(9.4 MB)\u001b[0m           │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_4          │ \u001b[2mfloat32\u001b[0m[1,7,7,512]     │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_6          │ \u001b[2mfloat32\u001b[0m[1,4096]        │ bias: \u001b[2mfloat32\u001b[0m[4096]          │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[25088,4096]  │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m102,764,544 \u001b[0m\u001b[1;2m(411.1 MB)\u001b[0m       │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8          │ \u001b[2mfloat32\u001b[0m[1,4096]        │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9          │ \u001b[2mfloat32\u001b[0m[1,4096]        │ bias: \u001b[2mfloat32\u001b[0m[4096]          │\n",
              "│                       │                        │ kernel: \u001b[2mfloat32\u001b[0m[4096,4096]   │\n",
              "│                       │                        │                              │\n",
              "│                       │                        │ \u001b[1m16,781,312 \u001b[0m\u001b[1;2m(67.1 MB)\u001b[0m         │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ net                   │ \u001b[2mfloat32\u001b[0m[1,10]          │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│ VGG                   │ \u001b[2mfloat32\u001b[0m[1,10]          │                              │\n",
              "├───────────────────────┼────────────────────────┼──────────────────────────────┤\n",
              "│\u001b[1m \u001b[0m\u001b[1m                     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                 Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m128,806,154 \u001b[0m\u001b[1;2m(515.2 MB)\u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\n",
              "└───────────────────────┴────────────────────────┴──────────────────────────────┘\n",
              "\u001b[1m                                                                                 \u001b[0m\n",
              "\u001b[1m                    Total Parameters: 128,806,154 \u001b[0m\u001b[1;2m(515.2 MB)\u001b[0m\u001b[1m                     \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rngs = {'params': random.PRNGKey(0), 'dropout': random.PRNGKey(1)}\n",
        "\n",
        "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
        "    (1, 224, 224, 1), rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3182f5",
      "metadata": {
        "origin_pos": 12
      },
      "source": [
        "As you can see, we halve height and width at each block,\n",
        "finally reaching a height and width of 7\n",
        "before flattening the representations\n",
        "for processing by the fully connected part of the network.\n",
        "\n",
        "## Training\n",
        "\n",
        "[**Since VGG-11 is more computationally-heavy than AlexNet\n",
        "we construct a network with a smaller number of channels.**]\n",
        "This is more than sufficient for training on Fashion-MNIST.\n",
        "The [**model training**] process is similar to that of AlexNet in :numref:`sec_alexnet`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413d18d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
        "# model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d1b75d",
      "metadata": {
        "origin_pos": 15
      },
      "source": [
        "## Summary\n",
        "\n",
        "One might argue that VGG is the first truly modern convolutional neural network. While AlexNet introduced many of the components of what make deep learning effective at scale, it is VGG that arguably introduced key properties such as blocks of multiple convolutions and a preference for deep and narrow networks. It is also the first network that is actually an entire family of similarly parametrized models, giving the practitioner ample trade-off between complexity and speed. This is also the place where modern deep learning frameworks shine. It is no longer necessary to generate XML config files to specify a network but rather, to assmple said networks through simple Python code. \n",
        "\n",
        "Very recently ParNet :cite:`Goyal.Bochkovskiy.Deng.ea.2021` demonstrated that it is possible to achieve competitive performance using a much more shallow architecture through a large number of parallel computations. This is an exciting development and there's hope that it will influence architecture designs in the future. For the remainder of the chapter, though, we will follow the path of scientific progress over the past decade. \n",
        "\n",
        "## Exercises\n",
        "\n",
        "\n",
        "1. Compared with AlexNet, VGG is much slower in terms of computation, and it also needs more GPU memory. \n",
        "    1. Compare the number of parameters needed for AlexNet and VGG.\n",
        "    1. Compare the number of floating point operations used in the convolutional layers and in the fully connected layers. \n",
        "    1. How could you reduce the computational cost created by the fully connected layers?\n",
        "1. When displaying the dimensions associated with the various layers of the network, we only see the information \n",
        "   associated with 8 blocks (plus some auxiliary transforms), even though the network has 11 layers. Where did \n",
        "   the remaining 3 layers go?\n",
        "1. Upsampling the resolution in Fashion-MNIST by a factor of $8 \\times 8$ from 28 to 224 dimensions is highly \n",
        "   wasteful. Try modifying the network architecture and resolution conversion, e.g., to 56 or to 84 dimensions \n",
        "   for its input instead. Can you do so without reducing the accuracy of the network?\n",
        "1. Use Table 1 in the VGG paper :cite:`Simonyan.Zisserman.2014` to construct other common models, \n",
        "   such as VGG-16 or VGG-19.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "423a4f32",
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/78)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "626d743d6476408aa1b36c3ff0d1f9d9d03e37c6879626ddfcdd13d658004bbf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
