{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "51ebe6c4",
      "metadata": {},
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "28ae0461",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import mock_d2l_jax as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1391537",
      "metadata": {
        "origin_pos": 1
      },
      "source": [
        "# Multi-Branch Networks  (GoogLeNet)\n",
        ":label:`sec_googlenet`\n",
        "\n",
        "In 2014, *GoogLeNet*\n",
        "won the ImageNet Challenge :cite:`Szegedy.Liu.Jia.ea.2015`, using a structure\n",
        "that combined the strengths of NiN :cite:`Lin.Chen.Yan.2013`, repeated blocks :cite:`Simonyan.Zisserman.2014`,\n",
        "and a cocktail of convolution kernels. It is arguably also the first network that exhibits a clear distinction among the stem, body, and head in a CNN. This design pattern has persisted ever since in the design of deep networks: the *stem* is given by the first 2-3 convolutions that operate on the image. They extract low-level features from the underlying images. This is followed by a *body* of convolutional blocks. Finally, the *head* maps the features obtained so far to the required classification, segmentation, detection, or tracking problem at hand.\n",
        "\n",
        "The key contribution in GoogLeNet was the design of the network body. It solved the problem of selecting\n",
        "convolution kernels in an ingenious way. While other works tried to identify which convolution, ranging from $1 \\times 1$ to $11 \\times 11$ would be best, it simply *concatenated* multi-branch convolutions.\n",
        "In what follows we introduce a slightly simplified version of GoogLeNet. The simplifications are due to the fact that  tricks to stabilize training, in particular intermediate loss functions, are no longer needed due to the availability of improved training algorithms.\n",
        "\n",
        "## (**Inception Blocks**)\n",
        "\n",
        "The basic convolutional block in GoogLeNet is called an *Inception block*,\n",
        "stemming from the meme \"we need to go deeper\" of the movie *Inception*.\n",
        "\n",
        "![Structure of the Inception block.](http://d2l.ai/_images/inception.svg)\n",
        ":label:`fig_inception`\n",
        "\n",
        "As depicted in :numref:`fig_inception`,\n",
        "the inception block consists of four parallel branches.\n",
        "The first three branches use convolutional layers\n",
        "with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$\n",
        "to extract information from different spatial sizes.\n",
        "The middle two branches also add a $1\\times 1$ convolution of the input\n",
        "to reduce the number of channels, reducing the model's complexity.\n",
        "The fourth branch uses a $3\\times 3$ max-pooling layer,\n",
        "followed by a $1\\times 1$ convolutional layer\n",
        "to change the number of channels.\n",
        "The four branches all use appropriate padding to give the input and output the same height and width.\n",
        "Finally, the outputs along each branch are concatenated\n",
        "along the channel dimension and comprise the block's output.\n",
        "The commonly-tuned hyperparameters of the Inception block\n",
        "are the number of output channels per layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "2b200026",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:23.397088Z",
          "iopub.status.busy": "2022-07-13T08:32:23.396441Z",
          "iopub.status.idle": "2022-07-13T08:32:25.976449Z",
          "shell.execute_reply": "2022-07-13T08:32:25.975530Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax import numpy as jnp, random, grad, vmap, jit\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "# from d2l import jax as d2l\n",
        "\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    # `c1`--`c4` are the number of output channels for each branch\n",
        "    c1: int\n",
        "    c2: tuple\n",
        "    c3: tuple\n",
        "    c4: int\n",
        "    \n",
        "    def setup(self):\n",
        "        # Branch 1\n",
        "        self.b1_1 = nn.Conv(self.c1, kernel_size=(1, 1))\n",
        "        # Branch 2\n",
        "        self.b2_1 = nn.Conv(self.c2[0], kernel_size=(1, 1))\n",
        "        self.b2_2 = nn.Conv(self.c2[1], kernel_size=(3, 3), padding='same')\n",
        "        # Branch 3\n",
        "        self.b3_1 = nn.Conv(self.c3[0], kernel_size=(1, 1))\n",
        "        self.b3_2 = nn.Conv(self.c3[1], kernel_size=(5, 5), padding='same')\n",
        "        # Branch 4\n",
        "        self.b4_1 = lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(1, 1), padding='same')\n",
        "        self.b4_2 = nn.Conv(self.c4, kernel_size=(1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        b1 = nn.relu(self.b1_1(x))\n",
        "        b2 = nn.relu(self.b2_2(nn.relu(self.b2_1(x))))\n",
        "        b3 = nn.relu(self.b3_2(nn.relu(self.b3_1(x))))\n",
        "        b4 = nn.relu(self.b4_2(self.b4_1(x)))\n",
        "        return jnp.concatenate((b1, b2, b3, b4), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9737eb1c",
      "metadata": {
        "origin_pos": 5
      },
      "source": [
        "To gain some intuition for why this network works so well,\n",
        "consider the combination of the filters.\n",
        "They explore the image in a variety of filter sizes.\n",
        "This means that details at different extents\n",
        "can be recognized efficiently by filters of different sizes.\n",
        "At the same time, we can allocate different amounts of parameters\n",
        "for different filters.\n",
        "\n",
        "\n",
        "## [**GoogLeNet Model**]\n",
        "\n",
        "As shown in :numref:`fig_inception_full`, GoogLeNet uses a stack of a total of 9 inception blocks, arranged into 3 groups with max-pooling in between,\n",
        "and global average pooling in its head to generate its estimates.\n",
        "Max-pooling between inception blocks reduces the dimensionality.\n",
        "At its stem, the first module is similar to AlexNet and LeNet.\n",
        "\n",
        "![The GoogLeNet architecture.](../img/inception-full.svg)\n",
        ":label:`fig_inception_full`\n",
        "\n",
        "We can now implement GoogLeNet piece by piece. Let's begin with the stem.\n",
        "The first module uses a 64-channel $7\\times 7$ convolutional layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "56fc5602",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:25.981037Z",
          "iopub.status.busy": "2022-07-13T08:32:25.980339Z",
          "iopub.status.idle": "2022-07-13T08:32:25.985285Z",
          "shell.execute_reply": "2022-07-13T08:32:25.984496Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "class GoogleNet(d2l.Classifier):\n",
        "    lr: float = 0.1\n",
        "    num_classes: int = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "e8a30f8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b1(self):\n",
        "    return [\n",
        "        nn.Conv(64, kernel_size=(7, 7), strides=(2, 2), padding='same'), nn.relu,\n",
        "        lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='same')\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d7a5e2",
      "metadata": {
        "origin_pos": 7
      },
      "source": [
        "The second module uses two convolutional layers:\n",
        "first, a 64-channel $1\\times 1$ convolutional layer,\n",
        "followed by a $3\\times 3$ convolutional layer that triples the number of channels. This corresponds to the second branch in the Inception block and concludes the design of the body. At this point we have 192 channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "e8bb91d2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:25.988809Z",
          "iopub.status.busy": "2022-07-13T08:32:25.988260Z",
          "iopub.status.idle": "2022-07-13T08:32:25.992969Z",
          "shell.execute_reply": "2022-07-13T08:32:25.992187Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b2(self):\n",
        "    return [\n",
        "        nn.Conv(64, kernel_size=(3, 3)), nn.relu,\n",
        "        nn.Conv(192, kernel_size=(3, 3), padding='same'), nn.relu,\n",
        "        lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='same')\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d7e853",
      "metadata": {
        "origin_pos": 9
      },
      "source": [
        "The third module connects two complete Inception blocks in series.\n",
        "The number of output channels of the first Inception block is\n",
        "$64+128+32+32=256$. This amounts to \n",
        "a ratio of the number of output channels\n",
        "among the four branches of $2:4:1:1$. Achieving this, we first reduce the input\n",
        "dimensions by $\\frac{1}{2}$ and by $\\frac{1}{12}$ in the second and third branch respectively\n",
        "to arrive at $96 = 192/2$ and $16 = 192/12$ channels respectively.\n",
        "\n",
        "The number of output channels of the second Inception block\n",
        "is increased to $128+192+96+64=480$, yielding a ratio of $128:192:96:64 = 4:6:3:2$. As before,\n",
        "we need to reduce the number of intermediate dimensions in the second and third channel. A\n",
        "scale of $\\frac{1}{2}$ and $\\frac{1}{8}$ respectively suffices, yielding $128$ and $32$ channels\n",
        "respectively. This is captured by the arguments of the following `Inception` block constructors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "a480a5d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:25.996541Z",
          "iopub.status.busy": "2022-07-13T08:32:25.996009Z",
          "iopub.status.idle": "2022-07-13T08:32:26.000734Z",
          "shell.execute_reply": "2022-07-13T08:32:25.999952Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b3(self):\n",
        "    return [Inception(64, (96, 128), (16, 32), 32),\n",
        "            Inception(128, (128, 192), (32, 96), 64),\n",
        "            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='same')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ecd9ad3",
      "metadata": {
        "origin_pos": 11
      },
      "source": [
        "The fourth module is more complicated.\n",
        "It connects five Inception blocks in series,\n",
        "and they have $192+208+48+64=512$, $160+224+64+64=512$,\n",
        "$128+256+64+64=512$, $112+288+64+64=528$,\n",
        "and $256+320+128+128=832$ output channels, respectively.\n",
        "The number of channels assigned to these branches is similar\n",
        "to that in the third module:\n",
        "the second branch with the $3\\times 3$ convolutional layer\n",
        "outputs the largest number of channels,\n",
        "followed by the first branch with only the $1\\times 1$ convolutional layer,\n",
        "the third branch with the $5\\times 5$ convolutional layer,\n",
        "and the fourth branch with the $3\\times 3$ max-pooling layer.\n",
        "The second and third branches will first reduce\n",
        "the number of channels according to the ratio.\n",
        "These ratios are slightly different in different Inception blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "7a709fd2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:26.003970Z",
          "iopub.status.busy": "2022-07-13T08:32:26.003671Z",
          "iopub.status.idle": "2022-07-13T08:32:26.009875Z",
          "shell.execute_reply": "2022-07-13T08:32:26.008814Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b4(self):\n",
        "    return [Inception(192, (96, 208), (16, 48), 64),\n",
        "            Inception(160, (112, 224), (24, 64), 64),\n",
        "            Inception(128, (128, 256), (24, 64), 64),\n",
        "            Inception(112, (144, 288), (32, 64), 64),\n",
        "            Inception(256, (160, 320), (32, 128), 128),\n",
        "            lambda x: nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='same')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5dd8ee0",
      "metadata": {
        "origin_pos": 13
      },
      "source": [
        "The fifth module has two Inception blocks with $256+320+128+128=832$\n",
        "and $384+384+128+128=1024$ output channels.\n",
        "The number of channels assigned to each branch\n",
        "is the same as that in the third and fourth modules,\n",
        "but differs in specific values.\n",
        "It should be noted that the fifth block is followed by the output layer.\n",
        "This block uses the global average pooling layer\n",
        "to change the height and width of each channel to 1, just as in NiN.\n",
        "Finally, we turn the output into a two-dimensional array\n",
        "followed by a fully connected layer\n",
        "whose number of outputs is the number of label classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "9af3ec78",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:26.015544Z",
          "iopub.status.busy": "2022-07-13T08:32:26.014621Z",
          "iopub.status.idle": "2022-07-13T08:32:26.019713Z",
          "shell.execute_reply": "2022-07-13T08:32:26.018861Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def b5(self):\n",
        "    return [Inception(256, (160, 320), (32, 128), 128),\n",
        "            Inception(384, (192, 384), (48, 128), 128),\n",
        "            lambda x: nn.avg_pool(x, (1, 1)),# TODO: nn.AdaptiveAvgPool2d((1,1)),\n",
        "            d2l.flatten]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "e8f48733",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:26.024405Z",
          "iopub.status.busy": "2022-07-13T08:32:26.024022Z",
          "iopub.status.idle": "2022-07-13T08:32:26.029649Z",
          "shell.execute_reply": "2022-07-13T08:32:26.028701Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(GoogleNet)\n",
        "def setup(self):\n",
        "    self.net = nn.Sequential([*self.b1(), *self.b2(), *self.b3(), *self.b4(),\n",
        "                              *self.b5(), nn.Dense(self.num_classes)])\n",
        "    # self.net.apply(d2l.init_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0123c232",
      "metadata": {
        "origin_pos": 16
      },
      "source": [
        "The GoogLeNet model is computationally complex. Note the large number of\n",
        "relatively arbitrary hyperparameters in terms of the number of channels chosen.\n",
        "This work was done before scientists started using automatic tools to\n",
        "optimize network designs.\n",
        "\n",
        "For now the only modification we will carry out is to\n",
        "[**reduce the input height and width from 224 to 96\n",
        "to have a reasonable training time on Fashion-MNIST.**]\n",
        "This simplifies the computation. Let's have a look at the\n",
        "changes in the shape of the output between the various modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "e706eae7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:26.033287Z",
          "iopub.status.busy": "2022-07-13T08:32:26.033005Z",
          "iopub.status.idle": "2022-07-13T08:32:26.205671Z",
          "shell.execute_reply": "2022-07-13T08:32:26.204352Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                             GoogleNet Summary                              </span>\n",
              "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> path               </span>┃<span style=\"font-weight: bold\"> outputs              </span>┃<span style=\"font-weight: bold\"> params                       </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Inputs             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,96,96,1]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_0       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,48,48,64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[7,7,1,64]    │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">3,200 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(12.8 KB)</span>              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,192]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[192]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,480,192] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">92,352 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(369.4 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,96]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[96]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,480,96]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">46,176 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(184.7 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,208]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[208]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,96,208]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">179,920 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(719.7 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,16]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,480,16]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">7,696 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(30.8 KB)</span>              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,48]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[48]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,16,48]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">19,248 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(77.0 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,480,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">30,784 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(123.1 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,512]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,160]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[160]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,160] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">82,080 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(328.3 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,112]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[112]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,112] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">57,456 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(229.8 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,224]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[224]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,112,224] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">226,016 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(904.1 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,24]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[24]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,24]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">12,312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(49.2 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,24,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">38,464 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(153.9 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">32,832 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(131.3 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,512]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">65,664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(262.7 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">65,664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(262.7 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,256]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,128,256] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">295,168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.2 MB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,24]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[24]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,24]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">12,312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(49.2 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,24,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">38,464 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(153.9 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">32,832 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(131.3 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,512]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,112]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[112]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,112] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">57,456 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(229.8 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,144]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[144]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,144] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">73,872 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(295.5 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,288]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[288]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,144,288] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">373,536 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.5 MB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,32]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">16,416 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(65.7 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,32,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">51,264 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(205.1 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,64]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,512,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">32,832 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(131.3 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,528]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,256]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,528,256] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">135,424 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(541.7 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,160]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[160]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,528,160] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">84,640 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(338.6 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,320]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[320]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,160,320] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">461,120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.8 MB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,32]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,528,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">16,928 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(67.7 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,32,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">102,528 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(410.1 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,528,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">67,712 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(270.8 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,6,6,832]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,256]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,256] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">213,248 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(853.0 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,160]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[160]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,160] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">133,280 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(533.1 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,320]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[320]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,160,320] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">461,120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.8 MB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,32]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">26,656 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(106.6 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,32,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">102,528 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(410.1 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">106,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(426.5 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,832]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b1_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,384]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[384]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,384] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">319,872 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(1.3 MB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b2_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,192]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[192]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,192] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">159,936 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(639.7 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b2_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,384]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[384]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,192,384] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">663,936 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.7 MB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b3_1 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,48]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[48]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,48]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">39,984 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(159.9 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b3_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,48,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">153,728 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(614.9 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b4_2 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,128]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,832,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">106,624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(426.5 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,3,3,1024]  │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_21      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,10]        │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[10]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[9216,10]     │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">92,170 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(368.7 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,24,24,64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,64,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">36,928 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(147.7 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_5       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,24,24,192] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[192]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,64,192]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">110,784 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(443.1 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b1_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,192,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">12,352 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(49.4 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b2_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,96]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[96]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,192,96]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">18,528 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(74.1 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b2_2  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,96,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">110,720 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(442.9 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b3_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,16]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[16]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,192,16]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">3,088 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(12.4 KB)</span>              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b3_2  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,32]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,16,32]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">12,832 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(51.3 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b4_2  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,32]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,192,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">6,176 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(24.7 KB)</span>              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,256] │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b1_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,256,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">32,896 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(131.6 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b2_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,128] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,256,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">32,896 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(131.6 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b2_2  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,192] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[192]           │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,3,128,192] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">221,376 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(885.5 KB)</span>           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b3_1  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,32]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[32]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,256,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">8,224 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(32.9 KB)</span>              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b3_2  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,96]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[96]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[5,5,32,96]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">76,896 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(307.6 KB)</span>            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b4_2  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,64]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]            │\n",
              "│                    │                      │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,1,256,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ <span style=\"font-weight: bold\">16,448 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(65.8 KB)</span>             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,12,12,480] │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net                │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,10]        │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ GoogleNet          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1,10]        │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">                Total </span>│<span style=\"font-weight: bold\"> 6,092,218 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(24.4 MB)</span><span style=\"font-weight: bold\">          </span>│\n",
              "└────────────────────┴──────────────────────┴──────────────────────────────┘\n",
              "<span style=\"font-weight: bold\">                                                                            </span>\n",
              "<span style=\"font-weight: bold\">                   Total Parameters: 6,092,218 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(24.4 MB)</span><span style=\"font-weight: bold\">                    </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                             GoogleNet Summary                              \u001b[0m\n",
              "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mpath              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                      \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Inputs             │ \u001b[2mfloat32\u001b[0m[1,96,96,1]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_0       │ \u001b[2mfloat32\u001b[0m[1,48,48,64]  │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[7,7,1,64]    │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m3,200 \u001b[0m\u001b[1;2m(12.8 KB)\u001b[0m              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b1_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,192]   │ bias: \u001b[2mfloat32\u001b[0m[192]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,480,192] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m92,352 \u001b[0m\u001b[1;2m(369.4 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b2_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,96]    │ bias: \u001b[2mfloat32\u001b[0m[96]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,480,96]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m46,176 \u001b[0m\u001b[1;2m(184.7 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b2_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,208]   │ bias: \u001b[2mfloat32\u001b[0m[208]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,96,208]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m179,920 \u001b[0m\u001b[1;2m(719.7 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b3_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,16]    │ bias: \u001b[2mfloat32\u001b[0m[16]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,480,16]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m7,696 \u001b[0m\u001b[1;2m(30.8 KB)\u001b[0m              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b3_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,48]    │ bias: \u001b[2mfloat32\u001b[0m[48]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,16,48]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m19,248 \u001b[0m\u001b[1;2m(77.0 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11/b4_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,480,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m30,784 \u001b[0m\u001b[1;2m(123.1 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_11      │ \u001b[2mfloat32\u001b[0m[1,6,6,512]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b1_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,160]   │ bias: \u001b[2mfloat32\u001b[0m[160]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,160] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m82,080 \u001b[0m\u001b[1;2m(328.3 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b2_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,112]   │ bias: \u001b[2mfloat32\u001b[0m[112]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,112] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m57,456 \u001b[0m\u001b[1;2m(229.8 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b2_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,224]   │ bias: \u001b[2mfloat32\u001b[0m[224]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,112,224] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m226,016 \u001b[0m\u001b[1;2m(904.1 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b3_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,24]    │ bias: \u001b[2mfloat32\u001b[0m[24]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,24]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m12,312 \u001b[0m\u001b[1;2m(49.2 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b3_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,24,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m38,464 \u001b[0m\u001b[1;2m(153.9 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12/b4_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m32,832 \u001b[0m\u001b[1;2m(131.3 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_12      │ \u001b[2mfloat32\u001b[0m[1,6,6,512]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b1_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m65,664 \u001b[0m\u001b[1;2m(262.7 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b2_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m65,664 \u001b[0m\u001b[1;2m(262.7 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b2_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,256]   │ bias: \u001b[2mfloat32\u001b[0m[256]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,128,256] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m295,168 \u001b[0m\u001b[1;2m(1.2 MB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b3_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,24]    │ bias: \u001b[2mfloat32\u001b[0m[24]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,24]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m12,312 \u001b[0m\u001b[1;2m(49.2 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b3_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,24,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m38,464 \u001b[0m\u001b[1;2m(153.9 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13/b4_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m32,832 \u001b[0m\u001b[1;2m(131.3 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_13      │ \u001b[2mfloat32\u001b[0m[1,6,6,512]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b1_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,112]   │ bias: \u001b[2mfloat32\u001b[0m[112]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,112] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m57,456 \u001b[0m\u001b[1;2m(229.8 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b2_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,144]   │ bias: \u001b[2mfloat32\u001b[0m[144]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,144] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m73,872 \u001b[0m\u001b[1;2m(295.5 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b2_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,288]   │ bias: \u001b[2mfloat32\u001b[0m[288]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,144,288] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m373,536 \u001b[0m\u001b[1;2m(1.5 MB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b3_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,32]    │ bias: \u001b[2mfloat32\u001b[0m[32]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m16,416 \u001b[0m\u001b[1;2m(65.7 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b3_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,32,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m51,264 \u001b[0m\u001b[1;2m(205.1 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14/b4_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,64]    │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,512,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m32,832 \u001b[0m\u001b[1;2m(131.3 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_14      │ \u001b[2mfloat32\u001b[0m[1,6,6,528]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b1_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,256]   │ bias: \u001b[2mfloat32\u001b[0m[256]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,528,256] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m135,424 \u001b[0m\u001b[1;2m(541.7 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b2_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,160]   │ bias: \u001b[2mfloat32\u001b[0m[160]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,528,160] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m84,640 \u001b[0m\u001b[1;2m(338.6 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b2_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,320]   │ bias: \u001b[2mfloat32\u001b[0m[320]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,160,320] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m461,120 \u001b[0m\u001b[1;2m(1.8 MB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b3_1 │ \u001b[2mfloat32\u001b[0m[1,6,6,32]    │ bias: \u001b[2mfloat32\u001b[0m[32]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,528,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m16,928 \u001b[0m\u001b[1;2m(67.7 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b3_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,32,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m102,528 \u001b[0m\u001b[1;2m(410.1 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15/b4_2 │ \u001b[2mfloat32\u001b[0m[1,6,6,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,528,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m67,712 \u001b[0m\u001b[1;2m(270.8 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_15      │ \u001b[2mfloat32\u001b[0m[1,6,6,832]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b1_1 │ \u001b[2mfloat32\u001b[0m[1,3,3,256]   │ bias: \u001b[2mfloat32\u001b[0m[256]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,256] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m213,248 \u001b[0m\u001b[1;2m(853.0 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b2_1 │ \u001b[2mfloat32\u001b[0m[1,3,3,160]   │ bias: \u001b[2mfloat32\u001b[0m[160]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,160] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m133,280 \u001b[0m\u001b[1;2m(533.1 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b2_2 │ \u001b[2mfloat32\u001b[0m[1,3,3,320]   │ bias: \u001b[2mfloat32\u001b[0m[320]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,160,320] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m461,120 \u001b[0m\u001b[1;2m(1.8 MB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b3_1 │ \u001b[2mfloat32\u001b[0m[1,3,3,32]    │ bias: \u001b[2mfloat32\u001b[0m[32]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m26,656 \u001b[0m\u001b[1;2m(106.6 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b3_2 │ \u001b[2mfloat32\u001b[0m[1,3,3,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,32,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m102,528 \u001b[0m\u001b[1;2m(410.1 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17/b4_2 │ \u001b[2mfloat32\u001b[0m[1,3,3,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m106,624 \u001b[0m\u001b[1;2m(426.5 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_17      │ \u001b[2mfloat32\u001b[0m[1,3,3,832]   │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b1_1 │ \u001b[2mfloat32\u001b[0m[1,3,3,384]   │ bias: \u001b[2mfloat32\u001b[0m[384]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,384] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m319,872 \u001b[0m\u001b[1;2m(1.3 MB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b2_1 │ \u001b[2mfloat32\u001b[0m[1,3,3,192]   │ bias: \u001b[2mfloat32\u001b[0m[192]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,192] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m159,936 \u001b[0m\u001b[1;2m(639.7 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b2_2 │ \u001b[2mfloat32\u001b[0m[1,3,3,384]   │ bias: \u001b[2mfloat32\u001b[0m[384]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,192,384] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m663,936 \u001b[0m\u001b[1;2m(2.7 MB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b3_1 │ \u001b[2mfloat32\u001b[0m[1,3,3,48]    │ bias: \u001b[2mfloat32\u001b[0m[48]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,48]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m39,984 \u001b[0m\u001b[1;2m(159.9 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b3_2 │ \u001b[2mfloat32\u001b[0m[1,3,3,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,48,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m153,728 \u001b[0m\u001b[1;2m(614.9 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18/b4_2 │ \u001b[2mfloat32\u001b[0m[1,3,3,128]   │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,832,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m106,624 \u001b[0m\u001b[1;2m(426.5 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_18      │ \u001b[2mfloat32\u001b[0m[1,3,3,1024]  │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_21      │ \u001b[2mfloat32\u001b[0m[1,10]        │ bias: \u001b[2mfloat32\u001b[0m[10]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[9216,10]     │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m92,170 \u001b[0m\u001b[1;2m(368.7 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_3       │ \u001b[2mfloat32\u001b[0m[1,24,24,64]  │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,64,64]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m36,928 \u001b[0m\u001b[1;2m(147.7 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_5       │ \u001b[2mfloat32\u001b[0m[1,24,24,192] │ bias: \u001b[2mfloat32\u001b[0m[192]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,64,192]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m110,784 \u001b[0m\u001b[1;2m(443.1 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b1_1  │ \u001b[2mfloat32\u001b[0m[1,12,12,64]  │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,192,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m12,352 \u001b[0m\u001b[1;2m(49.4 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b2_1  │ \u001b[2mfloat32\u001b[0m[1,12,12,96]  │ bias: \u001b[2mfloat32\u001b[0m[96]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,192,96]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m18,528 \u001b[0m\u001b[1;2m(74.1 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b2_2  │ \u001b[2mfloat32\u001b[0m[1,12,12,128] │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,96,128]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m110,720 \u001b[0m\u001b[1;2m(442.9 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b3_1  │ \u001b[2mfloat32\u001b[0m[1,12,12,16]  │ bias: \u001b[2mfloat32\u001b[0m[16]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,192,16]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m3,088 \u001b[0m\u001b[1;2m(12.4 KB)\u001b[0m              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b3_2  │ \u001b[2mfloat32\u001b[0m[1,12,12,32]  │ bias: \u001b[2mfloat32\u001b[0m[32]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,16,32]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m12,832 \u001b[0m\u001b[1;2m(51.3 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8/b4_2  │ \u001b[2mfloat32\u001b[0m[1,12,12,32]  │ bias: \u001b[2mfloat32\u001b[0m[32]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,192,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m6,176 \u001b[0m\u001b[1;2m(24.7 KB)\u001b[0m              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_8       │ \u001b[2mfloat32\u001b[0m[1,12,12,256] │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b1_1  │ \u001b[2mfloat32\u001b[0m[1,12,12,128] │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,256,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m32,896 \u001b[0m\u001b[1;2m(131.6 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b2_1  │ \u001b[2mfloat32\u001b[0m[1,12,12,128] │ bias: \u001b[2mfloat32\u001b[0m[128]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,256,128] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m32,896 \u001b[0m\u001b[1;2m(131.6 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b2_2  │ \u001b[2mfloat32\u001b[0m[1,12,12,192] │ bias: \u001b[2mfloat32\u001b[0m[192]           │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[3,3,128,192] │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m221,376 \u001b[0m\u001b[1;2m(885.5 KB)\u001b[0m           │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b3_1  │ \u001b[2mfloat32\u001b[0m[1,12,12,32]  │ bias: \u001b[2mfloat32\u001b[0m[32]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,256,32]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m8,224 \u001b[0m\u001b[1;2m(32.9 KB)\u001b[0m              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b3_2  │ \u001b[2mfloat32\u001b[0m[1,12,12,96]  │ bias: \u001b[2mfloat32\u001b[0m[96]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[5,5,32,96]   │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m76,896 \u001b[0m\u001b[1;2m(307.6 KB)\u001b[0m            │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9/b4_2  │ \u001b[2mfloat32\u001b[0m[1,12,12,64]  │ bias: \u001b[2mfloat32\u001b[0m[64]            │\n",
              "│                    │                      │ kernel: \u001b[2mfloat32\u001b[0m[1,1,256,64]  │\n",
              "│                    │                      │                              │\n",
              "│                    │                      │ \u001b[1m16,448 \u001b[0m\u001b[1;2m(65.8 KB)\u001b[0m             │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net/layers_9       │ \u001b[2mfloat32\u001b[0m[1,12,12,480] │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ net                │ \u001b[2mfloat32\u001b[0m[1,10]        │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│ GoogleNet          │ \u001b[2mfloat32\u001b[0m[1,10]        │                              │\n",
              "├────────────────────┼──────────────────────┼──────────────────────────────┤\n",
              "│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m6,092,218 \u001b[0m\u001b[1;2m(24.4 MB)\u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m│\n",
              "└────────────────────┴──────────────────────┴──────────────────────────────┘\n",
              "\u001b[1m                                                                            \u001b[0m\n",
              "\u001b[1m                   Total Parameters: 6,092,218 \u001b[0m\u001b[1;2m(24.4 MB)\u001b[0m\u001b[1m                    \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = GoogleNet().layer_summary((1, 96, 96, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39dd3e5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-07-13T08:32:26.210319Z",
          "iopub.status.busy": "2022-07-13T08:32:26.209581Z",
          "iopub.status.idle": "2022-07-13T08:35:48.763614Z",
          "shell.execute_reply": "2022-07-13T08:35:48.762659Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "model = GoogleNet(lr=0.01)\n",
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
        "# model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
        "trainer.fit(model, data, rngs={'params': random.PRNGKey(0), 'dropout': random.PRNGKey(1)})\n",
        "# TODO: AssignSubModuleError: Submodule Conv must be defined in `setup()` or in a method wrapped in `@compact` (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.AssignSubModuleError)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1ba163",
      "metadata": {
        "origin_pos": 22
      },
      "source": [
        "## Discussion\n",
        "\n",
        "A key feature of GoogLeNet is that it is actually *cheaper* to compute than its predecessors\n",
        "while simultaneously providing improved accuracy. This marks the beginning of a much more deliberate\n",
        "network design that trades off the cost of evaluating a network with a reduction in errors. It also marks the beginning of experimentation at a block level with network design hyperparameters, even though it was entirely manual at the time. This is largely due to the fact that deep learning frameworks in 2015 still lacked much of the design flexibility\n",
        "that we now take for granted. Moreover, full network optimization is costly and at the time training on ImageNet still\n",
        "proved computationally challenging.\n",
        "\n",
        "Over the following sections we will encounter a number of design choices (e.g., batch normalization, residual connections, and channel grouping) that allow us to improve networks significantly. For now, you can be proud to have implemented what is arguably the first truly modern CNN.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. GoogLeNet was so successful that it went through a number of iterations. There are several iterations\n",
        "   of GoogLeNet that progressively improved speed and accuracy. Try to implement and run some of them.\n",
        "   They include the following:\n",
        "   1. Add a batch normalization layer :cite:`Ioffe.Szegedy.2015`, as described\n",
        "      later in :numref:`sec_batch_norm`.\n",
        "   1. Make adjustments to the Inception block (width, choice and order of convolutions), as described in\n",
        "      :cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n",
        "   1. Use label smoothing for model regularization, as described in\n",
        "      :cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`.\n",
        "   1. Make further adjustments to the Inception block by adding residual connection\n",
        "      :cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`, as described later in\n",
        "      :numref:`sec_resnet`.\n",
        "1. What is the minimum image size for GoogLeNet to work?\n",
        "1. Can you design a variant of GoogLeNet that works on Fashion-MNIST's native resolution of $28 \\times 28$ pixels? How would you need to change the stem, the body, and the head of the network, if anything at all?\n",
        "1. Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do the latter two network\n",
        "   architectures significantly reduce the model parameter size?\n",
        "1. Compare the amount of computation needed in GoogLeNet and AlexNet. How does this affect the design of an accelerator chip, e.g., in terms of memory size, amount of computation, and the benefit of specialized operations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf1b16ca",
      "metadata": {
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/82)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "626d743d6476408aa1b36c3ff0d1f9d9d03e37c6879626ddfcdd13d658004bbf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
