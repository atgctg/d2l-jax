{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663f7151",
   "metadata": {},
   "source": [
    "The following additional libraries are needed to run this\n",
    "notebook. Note that running on Colab is experimental, please report a Github\n",
    "issue if you have any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86388af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import mock_d2l_jax as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdba95e",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Designing Convolution Network Architectures\n",
    ":label:`sec_cnn-design`\n",
    "\n",
    "The 2010s has witnessed shift\n",
    "from *feature engineering* to *network engineering*\n",
    "in computer vision.\n",
    "Since AlexNet (:numref:`sec_alexnet`)\n",
    "beat conventional computer vision models on ImageNet,\n",
    "constructing very deep networks\n",
    "by stacking the same blocks,\n",
    "especially $3 \\times 3$ convolutions,\n",
    "has been popularized by VGG networks (:numref:`sec_vgg`).\n",
    "The network in network (:numref:`sec_nin`)\n",
    "adds local nonlinearities via $1 \\times 1$ convolutions\n",
    "and uses global average pooling\n",
    "to aggregate information\n",
    "across all locations.\n",
    "GoogLeNet (:numref:`sec_googlenet`)\n",
    "is a multi-branch network that\n",
    "combines the advantages from the\n",
    "VGG network\n",
    "and the network in network,\n",
    "where its Inception block\n",
    "adopts the strategy of\n",
    "concatenated parallel transformations.\n",
    "ResNets (:numref:`sec_resnet`)\n",
    "stack residual blocks,\n",
    "which are two-branch subnetworks\n",
    "using identity mapping in one branch.\n",
    "DenseNets (:numref:`sec_densenet`)\n",
    "generalize the residual architectures.\n",
    "Other notable architectures\n",
    "include\n",
    "MobileNets that use network learning to achieve high accuracy in\n",
    "resource-constrained settings :cite:`Howard.Sandler.Chu.ea.2019`,\n",
    "the Squeeze-and-Excitation Networks (SENets) that\n",
    "allow for efficient information transfer between channels\n",
    ":cite:`Hu.Shen.Sun.2018`,\n",
    "and EfficientNets :cite:`tan2019efficientnet`\n",
    "that scale up networks via neural architecture search.\n",
    "\n",
    "Specifically, *neural architecture search* (NAS) :cite:`zoph2016neural,liu2018darts`\n",
    "is the process of automating neural network architectures.\n",
    "Given a fixed search space,\n",
    "NAS uses a search strategy\n",
    "to automatically select\n",
    "an architecture within the search space\n",
    "based on the returned performance estimation.\n",
    "The outcome of NAS\n",
    "is a single network instance.\n",
    "\n",
    "Instead of focusing on designing such individual instances,\n",
    "an alternative approach\n",
    "is to *design network design spaces*\n",
    "that characterize populations of networks :cite:`Radosavovic.Kosaraju.Girshick.ea.2020`.\n",
    "This method\n",
    "combines the strength of manual design and NAS.\n",
    "Through semi-automatic procedures (like in NAS),\n",
    "designing network design spaces\n",
    "explores the structure aspect of network design\n",
    "from the initial *AnyNet* design space.\n",
    "It then proceeds to discover design principles (like in manual design)\n",
    "that lead to simple and regular networks: *RegNets*.\n",
    "Before shedding light on these design principles,\n",
    "let's start with\n",
    "the initial design space.\n",
    "\n",
    "## The AnyNet Design Space\n",
    "\n",
    "The initial design space is called *AnyNet*,\n",
    "a relatively unconstrained design space,\n",
    "where we can focus on\n",
    "exploring network structure\n",
    "assuming standard, fixed blocks such as ResNeXt (:numref:`subsec_resnext`).\n",
    "Specifically,\n",
    "the network structure\n",
    "includes\n",
    "elements\n",
    "such as the number of blocks\n",
    "and the number of output channels\n",
    "in each stage,\n",
    "and the number of groups (group width) and bottleneck ratio\n",
    "within\n",
    "each ResNeXt block.\n",
    "\n",
    "\n",
    "\n",
    "![The AnyNet design space. Besides the number of groups and bottleneck ratio within each block, design choices include depth $d_i$ and the number of output channels $w_i$ for any stage $i$.](http://d2l.ai/_images/anynet.svg)\n",
    ":label:`fig_anynet`\n",
    "\n",
    "The AnyNet design space\n",
    "is shown in :numref:`fig_anynet`.\n",
    "This network\n",
    "begins with a *stem*,\n",
    "followed by a *body* with $n$ stages of transformation,\n",
    "and a final *head*.\n",
    "More concretely,\n",
    "the network stem\n",
    "is a $3 \\times 3$ convolution with stride 2\n",
    "that halves the height and width of an input image.\n",
    "The network head\n",
    "is a global average pooling followed\n",
    "by a fully connected layer to predict\n",
    "the output class.\n",
    "Note that\n",
    "the network stem and head\n",
    "are kept fixed and simple,\n",
    "so that the design focus in\n",
    "on the network body that is central\n",
    "to performance.\n",
    "Specifically,\n",
    "the network body\n",
    "consists of $n$ stages of transformation\n",
    "($n$ is given),\n",
    "where stage $i$\n",
    "consists of $d_i$ ResNeXt blocks\n",
    "with $w_i$ output channels,\n",
    "and progressively\n",
    "halves height and width via the first block\n",
    "(setting `use_1x1conv=True, strides=2` in `d2l.ResNeXtBlock` in :numref:`subsec_resnext`).\n",
    "Let's further\n",
    "denote\n",
    "the bottleneck ratio and\n",
    "the number of groups (group width) \n",
    "within\n",
    "each ResNeXt block for stage $i$\n",
    "as $b_i$ and $g_i$, respectively.\n",
    "Overall,\n",
    "despite of the straightforward network structure,\n",
    "varying $b_i$, $g_i$, $w_i$, and $d_i$\n",
    "results in\n",
    "a vast number of\n",
    "possible networks in the AnyNet design space.\n",
    "\n",
    "\n",
    "To implement AnyNet,\n",
    "we first define its network stem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507c645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:36:22.996844Z",
     "iopub.status.busy": "2022-07-15T06:36:22.996251Z",
     "iopub.status.idle": "2022-07-15T06:36:28.997789Z",
     "shell.execute_reply": "2022-07-15T06:36:28.996898Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp, random, grad, vmap, jit\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "# from d2l import jax as d2l\n",
    "\n",
    "\n",
    "class AnyNet(d2l.Classifier):\n",
    "    arch: tuple\n",
    "    stem_channels: int\n",
    "    lr=0.1\n",
    "    num_classes=10\n",
    "\n",
    "    def stem(self, num_channels):\n",
    "        return nn.Sequential([\n",
    "            nn.Conv(num_channels, kernel_size=(3, 3), strides=(2, 2), padding=(1, 1)),\n",
    "            nn.BatchNorm(use_running_average=False), nn.relu])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d84fde",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "Each stage consists of `depth` ResNeXt blocks,\n",
    "where `num_channels` specifies the block width.\n",
    "Note that the first block halves the height and width of input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce70cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:36:29.001992Z",
     "iopub.status.busy": "2022-07-15T06:36:29.001438Z",
     "iopub.status.idle": "2022-07-15T06:36:29.006633Z",
     "shell.execute_reply": "2022-07-15T06:36:29.005896Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(AnyNet)\n",
    "def stage(self, depth, num_channels, groups, bot_mul):\n",
    "    blk = []\n",
    "    for i in range(depth):\n",
    "        if i == 0:\n",
    "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul,\n",
    "                use_1x1conv=True, strides=(2, 2)))\n",
    "        else:\n",
    "            blk.append(d2l.ResNeXtBlock(num_channels, groups, bot_mul))\n",
    "    return nn.Sequential(blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828886d",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "Putting the network stem, body, and head together,\n",
    "we complete the implementation of AnyNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc66dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:36:29.273264Z",
     "iopub.status.busy": "2022-07-15T06:36:29.272636Z",
     "iopub.status.idle": "2022-07-15T06:36:29.278665Z",
     "shell.execute_reply": "2022-07-15T06:36:29.277873Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(AnyNet)\n",
    "def setup(self):\n",
    "    self.net = nn.Sequential([self.stem(self.stem_channels)])\n",
    "    for i, s in enumerate(self.arch):\n",
    "        self.net.layers.append(self.stage(*s))\n",
    "    self.net.extend([nn.AdaptiveAvgPool2d((1, 1)), d2l.flatten, nn.Dense(self.num_classes)])\n",
    "    # self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bda6de",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## Constraining Design Spaces with Lower Error Distributions\n",
    "\n",
    "For any stage $i$ of AnyNet,\n",
    "the design choices are \n",
    "the bottleneck ratio $b_i$ \n",
    "and the number of groups $g_i$\n",
    "within each block,\n",
    "block width $w_i$,\n",
    "and depth $d_i$.\n",
    "The designing network design spaces\n",
    "process starts\n",
    "from relatively unconstrained\n",
    "network structure characterized\n",
    "by ($b_i$, $g_i$, $w_i$, $d_i$)\n",
    "in the initial AnyNet design space.\n",
    "Then this process\n",
    "progressively samples models\n",
    "from the input design space\n",
    "to evaluate the error distribution :cite:`radosavovic2019network`\n",
    "as a quality indicator\n",
    "to output a more constrained\n",
    "design space with simpler models that may have\n",
    "better quality. \n",
    "\n",
    "Let's detail\n",
    "this quality indicator for design spaces.\n",
    "Given $n$ models sampled from some design space,\n",
    "the *error empirical distribution function* $F(e)$\n",
    "measures the fraction of models\n",
    "with errors $e_i$ lower than $e$:\n",
    "\n",
    "$$F(e) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}(e_i < e).$$\n",
    "\n",
    "\n",
    "Starting from the initial unconstrained AnyNet design space ($\\text{AnyNetX}_A$ in :cite:`Radosavovic.Kosaraju.Girshick.ea.2020`),\n",
    "sharing the bottle network ratio $b_i = b$ for all stages $i$ results in a more constrained design space $\\text{AnyNetX}_B$.\n",
    "Sampling and training $n=500$ models from $\\text{AnyNetX}_A$ and $\\text{AnyNetX}_B$ each,\n",
    "left of :numref:`fig_regnet-paper-fig5`\n",
    "shows that both design spaces have similar quality.\n",
    "Since simpler is better,\n",
    "we continue to search from $\\text{AnyNetX}_B$\n",
    "by additionally sharing the number of groups $g_i = g$.\n",
    "This leads to a further simplified design space\n",
    "$\\text{AnyNetX}_C$ with virtually no change\n",
    "in error distributions (right of :numref:`fig_regnet-paper-fig5`).\n",
    "\n",
    "![Comparing error empirical distribution functions of design spaces. The legends show the min error and mean error. Sharing bottleneck ratio (from $\\text{AnyNetX}_A$ to  $\\text{AnyNetX}_B$) and sharing the number of groups (from $\\text{AnyNetX}_B$ to $\\text{AnyNetX}_C$) simplify the design space with virtually no change in error distributions (figure taken from :cite:`Radosavovic.Kosaraju.Girshick.ea.2020`).](../img/regnet-paper-fig5.png)\n",
    ":width:`600px`\n",
    ":label:`fig_regnet-paper-fig5`\n",
    "\n",
    "Investigating good and bad models from $\\text{AnyNetX}_C$ suggests that it may be useful to increase width across stages :cite:`Radosavovic.Kosaraju.Girshick.ea.2020`.\n",
    "Empirically, simplifying\n",
    "$\\text{AnyNetX}_C$ to $\\text{AnyNetX}_D$\n",
    "with $w_{i} \\leq w_{i+1}$\n",
    "improves the quality of design spaces (left of  :numref:`fig_regnet-paper-fig7`).\n",
    "Similarly,\n",
    "adding further constraints of $d_{i} \\leq d_{i+1}$\n",
    "to increase network depth across stages\n",
    "gives an even better $\\text{AnyNetX}_E$\n",
    "(right of :numref:`fig_regnet-paper-fig7`).\n",
    "\n",
    "![Comparing error empirical distribution functions of design spaces. The legends show the min error and mean error. Increasing network width across stages (from $\\text{AnyNetX}_C$ to  $\\text{AnyNetX}_D$) and increasing network depth across stages (from $\\text{AnyNetX}_D$ to $\\text{AnyNetX}_E$) simplify the design space with improved  error distributions (figure taken from :cite:`Radosavovic.Kosaraju.Girshick.ea.2020`).](../img/regnet-paper-fig7.png)\n",
    ":width:`600px`\n",
    ":label:`fig_regnet-paper-fig7`\n",
    "\n",
    "\n",
    "\n",
    "## RegNet\n",
    "\n",
    "The resulting $\\text{AnyNetX}_E$ design space\n",
    "consists of simple networks\n",
    "following easy-to-interpret design principles:\n",
    "\n",
    "* Share the bottle network ratio $b_i = b$ for all stages $i$;\n",
    "* Share the number of groups $g_i = g$ for all stages $i$;\n",
    "* Increase network width across stages: $w_{i} \\leq w_{i+1}$;\n",
    "* Increase network depth across stages: $d_{i} \\leq d_{i+1}$.\n",
    "\n",
    "Following these design principles, :cite:`Radosavovic.Kosaraju.Girshick.ea.2020` proposed quantized linear constraints to\n",
    "$w_i$ and $d_i$ increasing,\n",
    "leading to\n",
    "RegNetX using ResNeXt blocks\n",
    "and RegNetY that additionally uses operators from SENets :cite:`Hu.Shen.Sun.2018`.\n",
    "As an example,\n",
    "we implement a 32-layer RegNetX variant\n",
    "characterized by\n",
    "\n",
    "* $b_i = 1;$\n",
    "* $g_i = 16;$\n",
    "* $w_1 = 32, w_2=80;$\n",
    "* $d_1 = 4, d_2=6.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbb3ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:36:29.282231Z",
     "iopub.status.busy": "2022-07-15T06:36:29.281718Z",
     "iopub.status.idle": "2022-07-15T06:36:29.286686Z",
     "shell.execute_reply": "2022-07-15T06:36:29.285920Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class RegNet32(AnyNet):\n",
    "    lr: float = 0.1\n",
    "    num_classes: int = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ed979",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "We can see that each RegNet stage progressively reduces resolution and increases output channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eec30d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:36:29.290371Z",
     "iopub.status.busy": "2022-07-15T06:36:29.289654Z",
     "iopub.status.idle": "2022-07-15T06:36:29.370378Z",
     "shell.execute_reply": "2022-07-15T06:36:29.369488Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "stem_channels, groups, bot_mul = 32, 16, 1\n",
    "depths, channels = (4, 6), (32, 80)\n",
    "\n",
    "RegNet32(((depths[0], channels[0], groups, bot_mul),\n",
    "             (depths[1], channels[1], groups, bot_mul)),\n",
    "            stem_channels).layer_summary((1, 96, 96, 1)) # TODO: AttributeError: module 'mock_d2l_jax' has no attribute 'ResNeXtBlock'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f465b5",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## Training\n",
    "\n",
    "Training the 32-layer RegNet on the Fashion-MNIST dataset is just like before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae56c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:36:29.374388Z",
     "iopub.status.busy": "2022-07-15T06:36:29.373831Z",
     "iopub.status.idle": "2022-07-15T06:40:21.726422Z",
     "shell.execute_reply": "2022-07-15T06:40:21.725512Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "model = RegNet32(lr=0.05)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df0383",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## Discussion\n",
    "\n",
    "With desirable properties like locality and translation invariance (:numref:`sec_why-conv`)\n",
    "for vision,\n",
    "CNNs have been the dominant architectures in this area.\n",
    "Recently,\n",
    "transformers (:numref:`sec_transformer`) :cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021,touvron2021training`\n",
    "and MLPs :cite:`tolstikhin2021mlp`\n",
    "have also sparked research beyond\n",
    "the well-established CNN architectures for vision.\n",
    "Specifically,\n",
    "although lacking of the aforementioned\n",
    "inductive biases inherent to CNNs,\n",
    "vision transformers (:numref:`sec_vision-transformer`)\n",
    "attained state-of-the-art performance\n",
    "in large-scale image classification in early 2020s,\n",
    "showing that\n",
    "*scalability trumps inductive biases*\n",
    ":cite:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`.\n",
    "In other words,\n",
    "it is often possible to\n",
    "train large transformers\n",
    "to outperform large CNNs on large datasets.\n",
    "Inspired\n",
    "by the superior scaling behavior of\n",
    "transformers (:numref:`sec_large-pretraining-transformers`) with multi-head self-attention (:numref:`sec_multihead-attention`),\n",
    "the process of gradually\n",
    "improving from a standard ResNet architecture\n",
    "toward the design of a vision transformer\n",
    "leads to a family of CNNs called the ConvNeXt models\n",
    "that compete favorably with transformers for vision :cite:`liu2022convnet`.\n",
    "We refer the interested readers\n",
    "to CNN design discussions\n",
    "in the ConvNeXt paper :cite:`liu2022convnet`.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Increase the number of stages to 4. Can you design a deeper RegNet that performs better?\n",
    "1. De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How does your new model perform?\n",
    "1. Implement multiple instances of a \"VioNet\" family by *violating* the design principles of RegNet. How do they perform? Which of ($d_i$, $w_i$, $g_i$, $b_i$) is the most important factor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43c36d",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/7463)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "626d743d6476408aa1b36c3ff0d1f9d9d03e37c6879626ddfcdd13d658004bbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
